{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "cell_id": "00002-f5e4b070-fa4e-4427-b310-05085ab2dd09",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 3863,
    "execution_start": 1635858068713,
    "source_hash": "6165965a",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Імпортуємо бібліотеки\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import re\n",
    "from spacy.tokens import DocBin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "cell_id": "00003-51c1b8f2-8900-4b74-81d1-291892f9206c",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 214,
    "execution_start": 1635858072581,
    "source_hash": "162d9b1c",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserName</th>\n",
       "      <th>ScreenName</th>\n",
       "      <th>Location</th>\n",
       "      <th>TweetAt</th>\n",
       "      <th>OriginalTweet</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3799</td>\n",
       "      <td>48751</td>\n",
       "      <td>London</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>@MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3800</td>\n",
       "      <td>48752</td>\n",
       "      <td>UK</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>advice Talk to your neighbours family to excha...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3801</td>\n",
       "      <td>48753</td>\n",
       "      <td>Vagabonds</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>Coronavirus Australia: Woolworths to give elde...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3802</td>\n",
       "      <td>48754</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>My food stock is not the only one which is emp...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3803</td>\n",
       "      <td>48755</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>Me, ready to go at supermarket during the #COV...</td>\n",
       "      <td>Extremely Negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UserName  ScreenName   Location     TweetAt  \\\n",
       "0      3799       48751     London  16-03-2020   \n",
       "1      3800       48752         UK  16-03-2020   \n",
       "2      3801       48753  Vagabonds  16-03-2020   \n",
       "3      3802       48754        NaN  16-03-2020   \n",
       "4      3803       48755        NaN  16-03-2020   \n",
       "\n",
       "                                       OriginalTweet           Sentiment  \n",
       "0  @MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...             Neutral  \n",
       "1  advice Talk to your neighbours family to excha...            Positive  \n",
       "2  Coronavirus Australia: Woolworths to give elde...            Positive  \n",
       "3  My food stock is not the only one which is emp...            Positive  \n",
       "4  Me, ready to go at supermarket during the #COV...  Extremely Negative  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Зчитуємо датасет (завантажено з https://www.kaggle.com/datatattle/covid-19-nlp-text-classification)\n",
    "trainDF = pd.read_csv(\"./Corona_NLP_train.csv\", encoding='ISO-8859-1')\n",
    "testDF = pd.read_csv(\"./Corona_NLP_test.csv\")\n",
    "trainDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "cell_id": "00005-e46ad648-9378-4f62-ad84-9a8473ded0b5",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 543,
    "execution_start": 1635858072800,
    "source_hash": "aab1244e",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 17982 Test: 3596 Valid: 900\n"
     ]
    }
   ],
   "source": [
    "# Реорганізуємо дані: \n",
    "# зменшуємо датасет (для швидшого тренування)\n",
    "# та робимо train-test-validation split\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "allDF = pd.concat((trainDF, testDF), ignore_index=True)\n",
    "allDF = allDF.sample(frac=0.5).reset_index(drop=True)\n",
    "\n",
    "trainDF, testDF = train_test_split(allDF, test_size = 0.2)\n",
    "testDF, validDF = train_test_split(testDF, test_size = 0.2)\n",
    "\n",
    "print(\"Train:\",len(trainDF), \"Test:\", len(testDF),\"Valid:\", len(validDF))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "allow_embed": "code",
    "cell_id": "00002-866ffbd9-556b-46db-864d-13b591162eb6",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 1,
    "execution_start": 1635858073345,
    "source_hash": "9bf797bf",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Препроцессинг: видаляємо посилання та кодуємо лейбли через one-hot encoding\n",
    "\n",
    "def remove_url(text): \n",
    "    parsed_text = re.sub(r\"\\S*https?:\\S*\", \"\", text, flags=re.MULTILINE)\n",
    "    return parsed_text\n",
    "\n",
    "def preprocess(df, embed):\n",
    "    df.OriginalTweet = df.OriginalTweet.apply(remove_url)\n",
    "    data = tuple(zip(df.OriginalTweet.tolist(), df.Sentiment.tolist())) \n",
    "    \n",
    "    # Перетворюємо потоки слів на вектори (embedding)\n",
    "    nlp = spacy.load(embed)\n",
    "    docs = []\n",
    "\n",
    "    for doc, label in nlp.pipe(data, as_tuples=True):\n",
    "        doc.cats['extremely_positive'] = 0\n",
    "        doc.cats['extremely_negative'] = 0\n",
    "        doc.cats['positive'] = 0\n",
    "        doc.cats['negative'] = 0\n",
    "        doc.cats['neutral']  = 0\n",
    "        \n",
    "        if label=='Extremely Positive':\n",
    "            doc.cats['extremely_positive'] = 1\n",
    "        elif label=='Positive':\n",
    "            doc.cats['extremely_negative'] = 1\n",
    "        elif label=='Neutral':\n",
    "            doc.cats['neutral']  = 1\n",
    "        elif label=='Negative':\n",
    "            doc.cats['negative'] = 1\n",
    "        else:\n",
    "            doc.cats['extremely_negative'] = 1\n",
    "        \n",
    "        docs.append(doc)\n",
    "        \n",
    "    return df, docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "allow_embed": "code_output",
    "cell_id": "00010-649bbc18-2053-47d5-a689-838939ad2569",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 9375,
    "execution_start": 1635724016177,
    "source_hash": "ff8f9ebb",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/redboot/.local/lib/python3.10/site-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "2022-12-02 09:10:30.168268: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\u001b[38;5;2m✔ Auto-filled config with all values\u001b[0m\n",
      "\u001b[38;5;2m✔ Saved config\u001b[0m\n",
      "config.cfg\n",
      "You can now add your data and train your pipeline:\n",
      "python -m spacy train config.cfg --paths.train ./train.spacy --paths.dev ./dev.spacy\n"
     ]
    }
   ],
   "source": [
    "# конфігурація: Використовуємо дефолтний згенерований конфіг spaCy для категоризації тексту\n",
    "# https://spacy.io/usage/training#quickstart\n",
    "!python -m spacy init fill-config ./base_config.cfg ./config.cfg "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "allow_embed": "code",
    "cell_id": "00009-605760bf-f627-4873-a2c6-22e477c4afa1",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 90229,
    "execution_start": 1635723925928,
    "source_hash": "c72c9fee",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# препроцессимо датасет та зберігаємо в файл .spacy\n",
    "\n",
    "train_data, train_docs = preprocess(trainDF, \"en_core_web_sm\")\n",
    "doc_bin = DocBin(docs=train_docs)\n",
    "doc_bin.to_disk(\"./textcat_train.spacy\")\n",
    "\n",
    "test_data, test_docs = preprocess(testDF, \"en_core_web_sm\")\n",
    "doc_bin = DocBin(docs=test_docs)\n",
    "doc_bin.to_disk(\"./textcat_test.spacy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "allow_embed": "output",
    "cell_id": "00015-31aa7384-e2c0-4db2-b720-bc8d7314e443",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 53058122,
    "execution_start": 1635724038829,
    "source_hash": "79c137b6",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/redboot/.local/lib/python3.10/site-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "2022-12-02 05:02:19.868313: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "[2022-12-02 05:02:21,974] [DEBUG] Config overrides from CLI: ['paths.train', 'paths.dev']\n",
      "\u001b[38;5;2m✔ Created output directory: ./textcat_output\u001b[0m\n",
      "\u001b[38;5;4mℹ Saving to output directory: ./textcat_output\u001b[0m\n",
      "\u001b[38;5;4mℹ Using CPU\u001b[0m\n",
      "\u001b[1m\n",
      "=========================== Initializing pipeline ===========================\u001b[0m\n",
      "[2022-12-02 05:02:22,287] [INFO] Set up nlp object from config\n",
      "[2022-12-02 05:02:22,299] [DEBUG] Loading corpus from path: ./textcat_test.spacy\n",
      "[2022-12-02 05:02:22,300] [DEBUG] Loading corpus from path: ./textcat_train.spacy\n",
      "[2022-12-02 05:02:22,300] [INFO] Pipeline: ['transformer', 'textcat']\n",
      "[2022-12-02 05:02:22,303] [INFO] Created vocabulary\n",
      "[2022-12-02 05:02:22,305] [INFO] Finished initializing nlp object\n",
      "Downloading config.json: 100%|██████████████████| 481/481 [00:00<00:00, 218kB/s]\n",
      "Downloading vocab.json: 100%|████████████████| 878k/878k [00:00<00:00, 1.52MB/s]\n",
      "Downloading merges.txt: 100%|█████████████████| 446k/446k [00:00<00:00, 913kB/s]\n",
      "Downloading tokenizer.json: 100%|██████████| 1.29M/1.29M [00:00<00:00, 2.15MB/s]\n",
      "Downloading pytorch_model.bin: 100%|█████████| 478M/478M [00:37<00:00, 13.4MB/s]\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[2022-12-02 05:03:22,230] [INFO] Initialized pipeline components: ['transformer', 'textcat']\n",
      "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
      "\u001b[1m\n",
      "============================= Training pipeline =============================\u001b[0m\n",
      "[2022-12-02 05:03:22,247] [DEBUG] Loading corpus from path: ./textcat_test.spacy\n",
      "[2022-12-02 05:03:22,248] [DEBUG] Loading corpus from path: ./textcat_train.spacy\n",
      "\u001b[38;5;4mℹ Pipeline: ['transformer', 'textcat']\u001b[0m\n",
      "\u001b[38;5;4mℹ Initial learn rate: 0.0\u001b[0m\n",
      "E    #       LOSS TRANS...  LOSS TEXTCAT  CATS_SCORE  SCORE \n",
      "---  ------  -------------  ------------  ----------  ------\n",
      "  0       0           0.00          2.60        0.00    0.00\n",
      "  0     200           0.00         63.25        0.00    0.00\n",
      "  1     400           0.17         58.71       29.87    0.30\n",
      "  1     600           0.60         50.54       41.98    0.42\n",
      "  1     800           0.61         52.42       49.28    0.49\n",
      "  2    1000           0.43         36.06       54.52    0.55\n",
      "  3    1200           1.20         42.47       55.77    0.56\n",
      "  3    1400           0.74         29.91       56.33    0.56\n",
      "  3    1600           0.70         28.39       57.41    0.57\n",
      "  4    1800           0.67         26.72       57.95    0.58\n",
      "  5    2000           1.01         29.96       59.06    0.59\n",
      "  5    2200           0.50         19.50       56.35    0.56\n",
      "  6    2400           0.63         25.54       62.61    0.63\n",
      "  6    2600           0.59         18.67       61.25    0.61\n",
      "  7    2800           0.78         23.83       60.90    0.61\n",
      "  7    3000           0.55         10.46       63.08    0.63\n",
      "  8    3200           0.59         14.70       61.78    0.62\n",
      "  8    3400           0.45          7.90       62.03    0.62\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "# Тренуємо модель: під капотом використовується tok2vec + bag-of-words + roberta\n",
    "!python -m spacy train ./config.cfg --verbose --output ./textcat_output --paths.train ./textcat_train.spacy --paths.dev ./textcat_test.spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "allow_embed": "code_output",
    "cell_id": "00030-fcc6e78e-8af5-474a-94f6-0cd55b1b2e6f",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 1687,
    "execution_start": 1635859851841,
    "source_hash": "515b878f",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: I hate when I go to the store to see food and toilet paper gone off the shelves people need to stop panic buying because it really got out a hand @JoshuaRush #Covid_19 #CoronavirusPandemic #coronavirus\n",
      "Original category: Extremely Negative\n",
      "Predicted:\n",
      "{'extremely_positive': 0.0007450793054886162, 'extremely_negative': 0.9142106771469116, 'positive': 7.710257705184631e-06, 'negative': 0.08471741527318954, 'neutral': 0.00031904916977509856}\n"
     ]
    }
   ],
   "source": [
    "# Валідуємо модель\n",
    "\n",
    "valid_data, valid_docs = preprocess(validDF, \"en_core_web_sm\")\n",
    "nlp_model = spacy.load(\"./textcat_output/model-best\")\n",
    "valid_text = valid_data.OriginalTweet.tolist()\n",
    "valid_cats = valid_data.Sentiment.tolist()\n",
    "doc_valid = nlp_model(valid_text[50])\n",
    "print(\"Text: \" + valid_text[50])\n",
    "print(\"Original category: \"+ valid_cats[50])\n",
    "print(\"Predicted:\") \n",
    "print(doc_valid.cats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "deepnote": {
   "is_reactive": false
  },
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "97cb8250-e42f-4e53-b2e7-ba29d30a97fd",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
